{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/BastienCherel/Advanced-ML-I/blob/main/Lab2_Ensemble_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5-SdPGosGEX"
   },
   "source": [
    "### Lab: Regression Task Using Random Forest, XGBoost, and LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ccx9iCQJsGEY"
   },
   "source": [
    "#### **Objective**:\n",
    "In this lab, you will learn how to apply three powerful ensemble learning algorithms—**Random Forest**, **XGBoost**, and **LightGBM**—to solve a regression problem. You will explore how to train and evaluate these models on a sample dataset, understand their strengths, and compare their performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MChFfxt4sGEY"
   },
   "source": [
    "#### **Prerequisites**:\n",
    "- Familiarity with Python and common ML libraries (`pandas`, `scikit-learn`).\n",
    "- Basic understanding of regression metrics such as Mean Squared Error (MSE) and R-squared.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8gGemFksGEY"
   },
   "source": [
    "#### **Libraries to Install**:\n",
    "Make sure you have the following libraries installed before running the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_Adt0fpsGEY"
   },
   "source": [
    "```bash\n",
    "# Install the required libraries\n",
    "!pip install pandas scikit-learn xgboost lightgbm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: xgboost in /home/codespace/.python/current/lib/python3.12/site-packages (2.1.3)\n",
      "Requirement already satisfied: lightgbm in /home/codespace/.python/current/lib/python3.12/site-packages (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/codespace/.python/current/lib/python3.12/site-packages (from xgboost) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas scikit-learn xgboost lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_Y6_6XIsGEY"
   },
   "source": [
    "### **1. Importing Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sdOFhl4isGEY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19vZlrgbsGEZ"
   },
   "source": [
    "### **2. Dataset: California Housing Prices**\n",
    "\n",
    "For this lab, we will use the **California Housing Prices** dataset, which is available from the `scikit-learn` dataset module. This dataset contains features like average income, house age, and house prices in various districts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJO1CMZpsGEZ"
   },
   "source": [
    "#### **Step 2.1: Load the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "M2p2S65BsGEZ",
    "outputId": "e182467d-723e-4f7e-e007-66f20ca4852e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  \n",
       "0    -122.23  \n",
       "1    -122.22  \n",
       "2    -122.24  \n",
       "3    -122.25  \n",
       "4    -122.25  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the dataset\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name=\"MedHouseVal\")\n",
    "\n",
    "# Display the first few rows\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXs0uAeSsGEZ"
   },
   "source": [
    "#### **Step 2.2: Split the Data**\n",
    "\n",
    "We will split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K_hXiuEcsGEa",
    "outputId": "e39f5a2e-d58b-4e46-9866-9c424cadfb3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (16512, 8), Testing data: (4128, 8)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data: {X_train.shape}, Testing data: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l33xZ_iSsGEa"
   },
   "source": [
    "### **3. Model 1: Random Forest Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aw5wCTaosGEa"
   },
   "source": [
    "#### **Step 3.1: Train the Random Forest Regressor**\n",
    "\n",
    "We’ll start with the **Random Forest Regressor**, which is an ensemble learning method that builds multiple decision trees and averages their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "U3wCMwnksGEa"
   },
   "outputs": [],
   "source": [
    "# Initialize the RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_rf = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "re6i7M5ysGEa"
   },
   "source": [
    "#### **Step 3.2: Evaluate the Random Forest Regressor**\n",
    "\n",
    "We will evaluate the performance of the Random Forest model using **Mean Squared Error (MSE)** and **R-squared (R²)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "setwfRtNsGEa",
    "outputId": "2152ac4d-52b6-4b73-e33d-966318cd36d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest MSE: 0.2554\n",
      "Random Forest R²: 0.8051\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest MSE: {mse_rf:.4f}\")\n",
    "print(f\"Random Forest R²: {r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tUvHt9asGEa"
   },
   "source": [
    "### **4. Model 2: XGBoost Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWt9Wbr3sGEb"
   },
   "source": [
    "#### **Step 4.1: Train the XGBoost Regressor**\n",
    "\n",
    "Next, we will train the **XGBoost** model, which uses gradient boosting techniques to optimize decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-vt2BZuXsGEb"
   },
   "outputs": [],
   "source": [
    "# Initialize the XGBoost regressor\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_xgb = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGBLWuUusGEb"
   },
   "source": [
    "#### **Step 4.2: Evaluate the XGBoost Regressor**\n",
    "We will now evaluate the performance of the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hx86oFB_sGEb",
    "outputId": "70eb0016-375e-4f0b-a5ad-ad69e6c3704a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MSE: 0.2273\n",
      "XGBoost R²: 0.8266\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost MSE: {mse_xgb:.4f}\")\n",
    "print(f\"XGBoost R²: {r2_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0z5rmNNasGEb"
   },
   "source": [
    "### **5. Model 3: LightGBM Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBPCZXo_sGEb"
   },
   "source": [
    "#### **Step 5.1: Train the LightGBM Regressor**\n",
    "\n",
    "Now, we will use **LightGBM**, another gradient boosting algorithm known for its speed and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mDub_p0BsGEb",
    "outputId": "ddff0546-6787-4e00-d99e-f83af030a73d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001691 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1838\n",
      "[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.071947\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LightGBM regressor\n",
    "lgb_model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_lgb = lgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90eOf-ScsGEb"
   },
   "source": [
    "#### **Step 5.2: Evaluate the LightGBM Regressor**\n",
    "\n",
    "Finally, evaluate the performance of the LightGBM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VwnE41MfsGEb",
    "outputId": "12de6f7a-5b27-41d5-e3f8-9c0d38741c3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM MSE: 0.2148\n",
      "LightGBM R²: 0.8360\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "mse_lgb = mean_squared_error(y_test, y_pred_lgb)\n",
    "r2_lgb = r2_score(y_test, y_pred_lgb)\n",
    "\n",
    "print(f\"LightGBM MSE: {mse_lgb:.4f}\")\n",
    "print(f\"LightGBM R²: {r2_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTaXCIsfsGEc"
   },
   "source": [
    "### **6. Comparing the Models**\n",
    "We will now compare the performance of the three models using **MSE** and **R²**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dSqd9yo_sGEc",
    "outputId": "e6dca89f-fc04-4bef-da3d-8bbe9387706f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison:\n",
      "Random Forest MSE: 0.2554, R²: 0.8051\n",
      "XGBoost MSE: 0.2273, R²: 0.8266\n",
      "LightGBM MSE: 0.2148, R²: 0.8360\n"
     ]
    }
   ],
   "source": [
    "# Print comparison of the three models\n",
    "print(\"Model Comparison:\")\n",
    "print(f\"Random Forest MSE: {mse_rf:.4f}, R²: {r2_rf:.4f}\")\n",
    "print(f\"XGBoost MSE: {mse_xgb:.4f}, R²: {r2_xgb:.4f}\")\n",
    "print(f\"LightGBM MSE: {mse_lgb:.4f}, R²: {r2_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEhRsVOMsGEc"
   },
   "source": [
    "### **7. Hyperparameter Tuning (if time is left)**\n",
    "\n",
    "For advanced users, you can improve model performance by tuning hyperparameters. Here’s an example of how to use GridSearchCV to tune hyperparameters for **Random Forest**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "eoCeKwjLsGEc",
    "outputId": "c1210ca4-ae70-4140-cdcf-71cb4dda94e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvh9m8AzsGEc"
   },
   "source": [
    "### **8. Individual Work**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skIJeDlQsGEc"
   },
   "source": [
    "#### **Exercises**:\n",
    "1. Experiment with the hyperparameters for **XGBoost** and **LightGBM**. Use `GridSearchCV` or `RandomizedSearchCV` to find optimal configurations.\n",
    "2. Try running the models on a different regression dataset (e.g., Boston Housing or any dataset of your choice).\n",
    "3. Analyze the training time of each model using the `time` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  \n",
       "0    -122.23  \n",
       "1    -122.22  \n",
       "2    -122.24  \n",
       "3    -122.25  \n",
       "4    -122.25  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the dataset\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name=\"MedHouseVal\")\n",
    "\n",
    "# Display the first few rows\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (16512, 8), Testing data: (4128, 8)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data: {X_train.shape}, Testing data: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter space for XGBoost\n",
    "xgb_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"subsample\": [0.8, 1],\n",
    "    \"colsample_bytree\": [0.8, 1]\n",
    "}\n",
    "\n",
    "# Hyperparameter space for LightGBM\n",
    "lgb_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"num_leaves\": [31, 50, 70],\n",
    "    \"min_child_samples\": [20, 30, 50]\n",
    "}\n",
    "\n",
    "# Define the Random Forest hyperparameter search space\n",
    "rf_param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [10, 20],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "lgb_model.set_params(**{\"verbose\": -1})\n",
    "rf_model = RandomForestRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning XGBoost...\n",
      "Tuning LightGBM...\n",
      "Tuning RandomForest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/numpy/ma/core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Use GridSearchCV for XGBoost\n",
    "print(\"Tuning XGBoost...\")\n",
    "start_time_xgb = time()\n",
    "xgb_grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=xgb_param_grid,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=3,\n",
    "    verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_grid_search.fit(X_train, y_train)\n",
    "xgb_best_model = xgb_grid_search.best_estimator_\n",
    "end_time_xgb = time()\n",
    "xgb_training_time = end_time_xgb - start_time_xgb\n",
    "\n",
    "\n",
    "# Use RandomizedSearchCV for LightGBM\n",
    "print(\"Tuning LightGBM...\")\n",
    "start_time_lgb = time()\n",
    "lgb_random_search = RandomizedSearchCV(\n",
    "    estimator=lgb_model,\n",
    "    param_distributions=lgb_param_grid,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_iter=50,\n",
    "    cv=3,\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lgb_random_search.fit(X_train, y_train)\n",
    "lgb_best_model = lgb_random_search.best_estimator_\n",
    "end_time_lgb = time()\n",
    "lgb_training_time = end_time_lgb - start_time_lgb\n",
    "\n",
    "\n",
    "# Use GridSearchCV for RandomForest\n",
    "print(\"Tuning RandomForest...\")\n",
    "start_time_rf = time()\n",
    "rf_random_search = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=rf_param_grid,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=3,\n",
    "    verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_random_search.fit(X_train, y_train)\n",
    "rf_best_model = rf_random_search.best_estimator_\n",
    "end_time_rf = time()\n",
    "rf_training_time = end_time_rf - start_time_rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models on the test set\n",
    "xgb_pred = xgb_best_model.predict(X_test)\n",
    "lgb_pred = lgb_best_model.predict(X_test)\n",
    "rf_pred = rf_best_model.predict(X_test)\n",
    "\n",
    "# Compute metrics\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))\n",
    "xgb_r2 = r2_score(y_test, xgb_pred)\n",
    "\n",
    "lgb_rmse = np.sqrt(mean_squared_error(y_test, lgb_pred))\n",
    "lgb_r2 = r2_score(y_test, lgb_pred)\n",
    "\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "rf_r2 = r2_score(y_test, rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- XGBoost Results ---\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 300, 'subsample': 1}\n",
      "RMSE: 0.4425\n",
      "R2 Score: 0.8506\n",
      "Training Time: 101.06 seconds\n",
      "\n",
      "--- LightGBM Results ---\n",
      "Best Parameters: {'num_leaves': 31, 'n_estimators': 300, 'min_child_samples': 30, 'max_depth': 7, 'learning_rate': 0.1}\n",
      "RMSE: 0.4413\n",
      "R2 Score: 0.8514\n",
      "Training Time: 23.46 seconds\n",
      "\n",
      "--- RandomForest Results ---\n",
      "Best Parameters: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "RMSE: 0.4941\n",
      "R2 Score: 0.8137\n",
      "Training Time: 948.44 seconds\n",
      "\n",
      "--- Comparison ---\n",
      "XGBoost Training Time: 101.06 seconds\n",
      "LightGBM Training Time: 23.46 seconds\n",
      "RandomForest Training Time: 948.44 seconds\n",
      "XGBoost RMSE: 0.4425 | LightGBM RMSE: 0.4413 | RandomForest RMSE: 0.4941\n",
      "XGBoost R2: 0.8506 | LightGBM R2: 0.8514 | RandomForest R2: 0.4941\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"\\n--- XGBoost Results ---\")\n",
    "print(f\"Best Parameters: {xgb_grid_search.best_params_}\")\n",
    "print(f\"RMSE: {xgb_rmse:.4f}\")\n",
    "print(f\"R2 Score: {xgb_r2:.4f}\")\n",
    "print(f\"Training Time: {xgb_training_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\n--- LightGBM Results ---\")\n",
    "print(f\"Best Parameters: {lgb_random_search.best_params_}\")\n",
    "print(f\"RMSE: {lgb_rmse:.4f}\")\n",
    "print(f\"R2 Score: {lgb_r2:.4f}\")\n",
    "print(f\"Training Time: {lgb_training_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\n--- RandomForest Results ---\")\n",
    "print(f\"Best Parameters: {rf_random_search.best_params_}\")\n",
    "print(f\"RMSE: {rf_rmse:.4f}\")\n",
    "print(f\"R2 Score: {rf_r2:.4f}\")\n",
    "print(f\"Training Time: {rf_training_time:.2f} seconds\")\n",
    "\n",
    "# Compare training times and metrics\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(f\"XGBoost Training Time: {xgb_training_time:.2f} seconds\")\n",
    "print(f\"LightGBM Training Time: {lgb_training_time:.2f} seconds\")\n",
    "print(f\"RandomForest Training Time: {rf_training_time:.2f} seconds\")\n",
    "print(f\"XGBoost RMSE: {xgb_rmse:.4f} | LightGBM RMSE: {lgb_rmse:.4f} | RandomForest RMSE: {rf_rmse:.4f}\")\n",
    "print(f\"XGBoost R2: {xgb_r2:.4f} | LightGBM R2: {lgb_r2:.4f} | RandomForest R2: {rf_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUxD9_ZFsGEc"
   },
   "source": [
    "#### **Research Points**:\n",
    "- How do the three models differ in terms of training time and performance?\n",
    "- Why might LightGBM or XGBoost outperform Random Forest on certain datasets?\n",
    "- How do the ensemble methods like Random Forest and boosting methods like XGBoost and LightGBM handle overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Comparison: Training Time and Performance\n",
    "\n",
    "Training Time:\n",
    "* LightGBM is significantly faster than XGBoost and Random Forest. This is due to LightGBM’s histogram-based learning, which speeds up feature splits by grouping continuous features into discrete bins.\n",
    "* Random Forest is the slowest because it grows many decision trees independently, each considering all features (or a subset) during training, leading to computational overhead.\n",
    "\n",
    "Performance (RMSE & R²):\n",
    "* LightGBM achieves the best performance (lowest RMSE and highest R²), indicating a slightly better fit compared to XGBoost.\n",
    "* Random Forest performs worst in both RMSE and R², suggesting it is less suited for the dataset in question.\n",
    "\n",
    "\n",
    "##### Why LightGBM and XGBoost Outperform Random Forest\n",
    "\n",
    "**Boosting vs Bagging:**\n",
    "\n",
    "Random Forest (Bagging):\n",
    "* Builds trees independently by sampling data with replacement (bootstrap aggregation).\n",
    "* Reduces variance but can struggle with bias if individual trees are weak.\n",
    "* May underperform on datasets with complex relationships due to lack of iterative learning.\n",
    "\n",
    "XGBoost and LightGBM (Boosting):\n",
    "* Build trees sequentially, each focusing on correcting the errors of the previous tree.\n",
    "* Reduce both bias and variance by iteratively improving predictions.\n",
    "* More effective at capturing complex patterns in data compared to Random Forest.\n",
    "\n",
    "**Efficiency of Boosting Algorithms:**\n",
    "\n",
    "XGBoost:\n",
    "* Implements regularization (L1 and L2) to prevent overfitting.\n",
    "* Weighted data points to prioritize misclassified samples.\n",
    "* Handles sparsity efficiently, which is beneficial for high-dimensional datasets.\n",
    "\n",
    "LightGBM:\n",
    "* Optimized for speed with histogram-based learning.\n",
    "* Handles large datasets and high-dimensional features more efficiently than XGBoost.\n",
    "* Splits leaf-wise rather than depth-wise, focusing on the most significant feature splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d66q3VBYsGEc"
   },
   "source": [
    "This notebook provides a hands-on approach to comparing **Random Forest**, **XGBoost**, and **LightGBM** for a regression task. It introduces the core concepts and provides insights into their performance, with opportunities for deeper exploration."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
